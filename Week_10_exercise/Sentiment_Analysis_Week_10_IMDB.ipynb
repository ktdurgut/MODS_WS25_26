{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMv1OaWgyy1N"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)  # Set seed for NumPy\n",
    "random.seed(42) # Set seed for random module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLuwyHbP9xrE"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Similarly to last week, in this week's tutorial we will again perform a **sentiment analysis**. The difference is that this time we want to learn the sentiment of words using **supervised learning** instead of looking them up in a dictionary.\n",
    "\n",
    "The dataset is the same as last week, so we are still trying to classify movie reviews (from IMDB) as positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The dataset we will use contains movie reviews from IMDB. Initially the data is stored as a dataframe with three columns (id, sentiment_human, text).\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#Loading the data from a csv file\n",
    "reviews = pd.read_csv(\"https://raw.githubusercontent.com/kbrennig/MODS_WS25_26/refs/heads/main/data/imdb_sample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35AXenNS-Bx8"
   },
   "source": [
    "Since we want to perform a classification afterward, we recode the output variable such that a positive sentiment is represented by 1 and a negative sentiment by 0. So it can be used in supervised learning models.\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l5M7lYhb9v0k"
   },
   "outputs": [],
   "source": [
    "# Recode sentiment_human\n",
    "reviews['sentiment_positive'] = np.where(reviews['sentiment_human'] == 'positive', 1, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nE3bWCC-Y5r"
   },
   "source": [
    "## Convert data to text format for processing\n",
    "We will use the text of the movie reviews that we used last week for further analysis.\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1730736221584,
     "user": {
      "displayName": "Arthur S",
      "userId": "06175255907682516412"
     },
     "user_tz": -60
    },
    "id": "403ArP8_-Jzv",
    "outputId": "0b3548b6-81d8-4699-e75f-ae0220396f68"
   },
   "outputs": [],
   "source": [
    "# Display the first row of the data\n",
    "reviews['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ki2NFJ1n-mU7"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "Since unstructured data doesn't have an inherent and consistent structure we have to perform some preprocessing steps in order to make the data usable for the computer.\n",
    "One thing to keep in mind is that the more preprocessing we perform the more information we lose, but the basic methods we are using here require it.\n",
    "\n",
    "### Tokenize documents\n",
    "First, we tokenize the texts. This means we transform the texts from one long string to a list of tokens. Additionally we also start removing unwanted characters (e.g punctuation between sentences, numbers, etc.). \n",
    "\n",
    "### Lemmatize all words\n",
    "After tokenizing the texts we perform lemmatization (alternatively stemming could be performed). Lemmatization replaces each word with its dictionary form ([lemma](https://en.wikipedia.org/wiki/Lemma_(morphology))).\n",
    "\n",
    "### Remove stopwords\n",
    "Finally we remove words that don't contain real meaning and are commonly used (e.g. 'this', 'the', 'a', etc.). \n",
    "\n",
    "\n",
    "Additionally, we remove unwanted characters (e.g., punctuation and numbers).\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1730736221584,
     "user": {
      "displayName": "Arthur S",
      "userId": "06175255907682516412"
     },
     "user_tz": -60
    },
    "id": "6Q9Ebphm4tOl",
    "outputId": "1cd670cd-9e02-48dd-a6ff-839bd353c675"
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download the punkt resource\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "#Define function with all necessary preprocessing steps for our IMDB reviews. In comparison to last week we now use Lemmatization instead of Stemming.\n",
    "def preprocess(text):\n",
    "    # tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # create lemmatizer object\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # lemmatize each token\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # get list of stopwords in English\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "    # remove stopwords\n",
    "    filtered_tokens = [token for token in lemmatized_tokens if token.lower() not in stopwords]\n",
    "    \n",
    "    # remove punctuation\n",
    "    filtered_tokens_nopunct = [token for token in filtered_tokens if token not in string.punctuation]\n",
    "\n",
    "    return  \" \".join(filtered_tokens_nopunct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply preprocessing\n",
    "\n",
    "After defining the different preprocessing steps, we now apply these preprocessing steps to our IMDB reviews. Running the code below we apply the preprocess function to the \"text\" column of our data and save the new preprocessed reviews as a new column in our dataset.\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "executionInfo": {
     "elapsed": 22371,
     "status": "ok",
     "timestamp": 1730736243952,
     "user": {
      "displayName": "Arthur S",
      "userId": "06175255907682516412"
     },
     "user_tz": -60
    },
    "id": "VTXYZJBp54jm",
    "outputId": "09ff0117-f237-4e1b-f502-2b7754527d3b"
   },
   "outputs": [],
   "source": [
    "# Apply text preprocessing\n",
    "reviews['processed_text'] = reviews['text'].apply(preprocess)\n",
    "reviews['processed_text'].iloc[0]  # Display first processed review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Xg6pnC4-snV"
   },
   "source": [
    "## Remove irrelevant words\n",
    "In this case, we manually remove specific words that are irrelevant to the analysis.\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mA13s6az9v6x"
   },
   "outputs": [],
   "source": [
    "# Remove additional irrelevant words (amp, document)\n",
    "reviews['processed_text'] = reviews['processed_text'].replace(['amp', 'document'], '', regex=True)\n",
    "reviews['processed_text'].iloc[0] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UbEZfWo-vhb"
   },
   "source": [
    "## Supervised Sentiment Analysis\n",
    "Last week we performed **dictionary-based Sentiment Analysis** where we used a dictionary to look up each word's sentiment. This week we want to learn the sentiment of the words contained in the reviews by using **supervised learning**. However, this needs additional dataset transformations which we will perform subsequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for classifier\n",
    "We split the data into a training and test set for supervised learning.\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Drop original review text from the data as we don't need it anymore\n",
    "reviews_preprocessed = reviews.drop(columns=\"text\")\n",
    "\n",
    "#define X and y\n",
    "X = reviews_preprocessed.drop(columns=['sentiment_positive'])\n",
    "y = reviews_preprocessed['sentiment_positive']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct document-term matrix for training set and test set\n",
    "We transform the training and the test data into a matrix where each row represents a text, and each column represents a word (token). We apply TF-IDF to the matrix. The cell (i,j) represents the number of occurrences of the j-th word in the i-th document. The resulting matrix has 5,000 rows (documents) and 33,071 columns (features/tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "executionInfo": {
     "elapsed": 1587,
     "status": "ok",
     "timestamp": 1730736245537,
     "user": {
      "displayName": "Arthur S",
      "userId": "06175255907682516412"
     },
     "user_tz": -60
    },
    "id": "DUyzOLOz9v9U",
    "outputId": "3480757a-949f-4905-975c-c1bc8ffaf5a5"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Vectorize the text using Term Frequency\n",
    "count_vectorizer = CountVectorizer() \n",
    "tf_vectorizer = count_vectorizer.fit(X_train['processed_text'])\n",
    "reviews_matrix_train = tf_vectorizer.transform(X_train['processed_text'])\n",
    "reviews_matrix_test = tf_vectorizer.transform(X_test['processed_text'])\n",
    "\n",
    "# Show the shape of the resulting matrix\n",
    "print(reviews_matrix_train.shape)\n",
    "print(reviews_matrix_test.shape)\n",
    "\n",
    "# Turn matrix into a DataFrame so it can be used for supervised learning and merge with sentiment\n",
    "reviews_matrix_train_df = pd.DataFrame(reviews_matrix_train.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "reviews_matrix_test_df = pd.DataFrame(reviews_matrix_test.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the first few rows of the training matrix\n",
    "reviews_matrix_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rare terms from the train and test matrix (min_df)\n",
    "\n",
    "In order to decrease the size of the matrix we filter out tokens that occur less than 15 times (columns whose sum is <15). This already decreases the number of columns from 33,071 to 3,927."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text using Term Frequency\n",
    "count_vectorizer = CountVectorizer(min_df=15) #define min_df\n",
    "tf_vectorizer = count_vectorizer.fit(X_train['processed_text'])\n",
    "reviews_matrix_train = tf_vectorizer.transform(X_train['processed_text'])\n",
    "reviews_matrix_test = tf_vectorizer.transform(X_test['processed_text'])\n",
    "\n",
    "# Show the shape of the resulting matrix\n",
    "print(reviews_matrix_train.shape)\n",
    "print(reviews_matrix_test.shape)\n",
    "\n",
    "# Turn matrix into a DataFrame so it can be used for supervised learning and merge with sentiment\n",
    "reviews_matrix_train_df = pd.DataFrame(reviews_matrix_train.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "reviews_matrix_test_df = pd.DataFrame(reviews_matrix_test.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the first few rows of the training matrix\n",
    "reviews_matrix_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform counts to TF-IDF weights\n",
    "\n",
    "In the previous step we used the **term frequency (TF)** to filter out very rare words since they most likely won't contribute a lot to our model. On the other hand terms that appear in almost every document are also most likely not so informative but TF isn't enough to determine that. That's why we need an additional metric called **inverse document frequency (IDF)** which takes on big values for terms that only appear in a few documents and small values for terms that appear very often. \n",
    "\n",
    "When we put these two metrics together we get the **TF-IDF = TF\\*IDF**. We use this formula to recalculate our matrix entries.\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Vectorize the text using TF-IDF\n",
    "vectorizer = TfidfVectorizer(min_df=15) #define min_df\n",
    "tfidf_vectorizer = vectorizer.fit(X_train['processed_text'])\n",
    "reviews_matrix_train_tfidf = tfidf_vectorizer.transform(X_train['processed_text'])\n",
    "reviews_matrix_test_tfidf = tfidf_vectorizer.transform(X_test['processed_text'])\n",
    "\n",
    "# Show the shape of the resulting matrix\n",
    "print(reviews_matrix_train_tfidf.shape)\n",
    "print(reviews_matrix_test_tfidf.shape)\n",
    "\n",
    "# Turn matrix into a DataFrame so it can be used for supervised learning and merge with sentiment\n",
    "reviews_matrix_train_df_tfidf = pd.DataFrame(reviews_matrix_train_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "reviews_matrix_test_df_tfidf = pd.DataFrame(reviews_matrix_test_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the first few rows of the training matrix\n",
    "reviews_matrix_train_df_tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CI5QnNWe-z-D"
   },
   "source": [
    "## Train and evaluate classifier\n",
    "From here on, almost everything is the same as before when we performed classification with the only difference being that the input features are now terms instead of numbers or categorical values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQ4q-p9l-5BA"
   },
   "source": [
    "### Train random forest classifier\n",
    "We train a random forest classifier onthe training set (without hyperparameter tuning) to classify the sentiment based on the processed text features.\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "executionInfo": {
     "elapsed": 7051,
     "status": "ok",
     "timestamp": 1730736252937,
     "user": {
      "displayName": "Arthur S",
      "userId": "06175255907682516412"
     },
     "user_tz": -60
    },
    "id": "AmXNDW4J-7qH",
    "outputId": "9955f08c-a80c-49cd-c209-cf9730b4765d"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "rf_01 = RandomForestClassifier(random_state=42).fit(reviews_matrix_train_df_tfidf, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are different parameters that have to be specified in order to train a random forest. The __Number of trees__ (n_estimators) for example tells us how many trees have been trained.   \n",
    "For a complete list of the `RandomForestClassifier` functions parameters, you can have a look at its [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n",
    "\n",
    "*Run the code below.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Parameters:\", rf_01.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCpYXYwL-7-G"
   },
   "source": [
    "### Make predictions and calculate evaluation metrics on test set\n",
    "\n",
    "Similarly to last week, we can make predictions on the test set and calculate different evaluation metrics.\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "predictions_testset_rf01 = rf_01.predict_proba(reviews_matrix_test_df_tfidf)[:, 1]\n",
    "predictions_testset_rf01_binary = np.where(predictions_testset_rf01 > 0.5, 1, 0)\n",
    "\n",
    "# Calculate Accuracy\n",
    "\n",
    "accuracy_rf = accuracy_score(y_test, predictions_testset_rf01_binary)\n",
    "print(\"Accuracy (Random Forests):\", accuracy_rf)\n",
    "\n",
    "# Create the confusion matrix\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, predictions_testset_rf01_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC and Auc\n",
    "\n",
    "Plot ROC curve and calculate AUC on test set.\n",
    "With __binary__ classification we get relatively straight lines. With the classification __probability__ we can map the distribution better. That is why we use the classification probability (e.g., predictions_testset_rf01) to calculate the AUC.\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "# Calculate and Print the AUC score\n",
    "auc_score = roc_auc_score(y_test, predictions_testset_rf01)\n",
    "print(\"AUC Score:\", auc_score)\n",
    "\n",
    "#plot ROC curve\n",
    "RocCurveDisplay.from_predictions(y_test, predictions_testset_rf01, plot_chance_level=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5JQyJ4D_QjT"
   },
   "source": [
    "# Summary\n",
    "In this tutorial, we:\n",
    "\n",
    "1. Preprocessed the text (tokenization, lemmatization, stopword removal, etc.).\n",
    "2. Made a Train Test Split of the data.\n",
    "2. Transformed the data into a matrix using TF-IDF.\n",
    "3. Performed supervised sentiment analysis using a random forest classifier.\n",
    "4. Evaluated the model using accuracy, confusion matrix, ROC curve, and AUC.\n",
    "\n",
    "## Perform additional analyses below\n",
    "\n",
    "You can use the cell below to perform and evaluate different sentiment analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bv6C7Khe_SVR"
   },
   "outputs": [],
   "source": [
    "# Enter your Code here!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMutoud+Q2i40TkP6BneRD2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
