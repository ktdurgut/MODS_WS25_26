{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "105de269",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(42)  # Set seed for NumPy\n",
        "random.seed(42) # Set seed for random module"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b3152e4",
      "metadata": {
        "id": "9b3152e4"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this weeks tutorial we will work with __Topic modeling__, which is an __unsupervised__ method for text analysis. As manual annotation of data is very time-consuming, naturally there is much more raw than annotated data.\n",
        "But we already know one method to analyze unlabeled data: __Clustering__. Topic modeling is a special case of clustering and is theoretically based on the \"Distributional hypothesis of linguistics\" which states that words that co-occur together in similar contexts tend to have similar meanings. Additionally these co-occurrence patterns can be interpreted as topics and used to cluster documents.\n",
        "\n",
        "We are still working with the same dataset as before (IMDB) and since this dataset contains annotations (sentiment scores), we will finally use the created topic model as new features for predicting the sentiment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfa17acb",
      "metadata": {},
      "source": [
        "## Data\n",
        "\n",
        "The dataset we will use contains movie reviews from IMDB. Initially the data is stored as a dataframe with three columns (id, sentiment_human, text).\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daa3283d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "#Loading the data from a csv file\n",
        "reviews = pd.read_csv(\"https://raw.githubusercontent.com/kbrennig/MODS_WS25_26/refs/heads/main/data/imdb_sample.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c972415",
      "metadata": {},
      "source": [
        "### Prepare data for classifier\n",
        "\n",
        "For our classifier we only need the ground truth sentiment_score (which we again recode from 'positive'/'negative' to 1/0) and the topic columns.\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c59d881b",
      "metadata": {},
      "outputs": [],
      "source": [
        "reviews['sentiment_positive'] = np.where(reviews['sentiment_human'] == 'positive', 1, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93bc696d",
      "metadata": {},
      "source": [
        "### Split Dataset into Train- and Testset\n",
        "\n",
        "We create a train- and testset so that we can later use the topics as input for a machine learning model.\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "196ef5a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#define X and y\n",
        "X = reviews.drop(columns=['id','sentiment_human','sentiment_positive'])\n",
        "y = reviews['sentiment_positive']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "y_train = y_train.reset_index(drop=True)\n",
        "y_test = y_test.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ff00fce",
      "metadata": {},
      "source": [
        "## Preprocessing\n",
        "\n",
        "Since unstructured data doesn't have an inherent and consistent structure we have to perform some preprocessing steps in order to make the data usable for the computer.\n",
        "One thing to keep in mind is that the more preprocessing we perform the more information we lose, but the basic methods we are using here require it.\n",
        "\n",
        "### Tokenize documents\n",
        "First, we tokenize the texts. This means we transform the texts from one long string to a list of tokens.\n",
        "\n",
        "### Stem all words\n",
        "After tokenizing the texts we perform stemming (alternatively lemmatization could be performed). Stemming reduces every word to its stem.\n",
        "The stemmer we use here is the Porter Stemmer.\n",
        "\n",
        "### Remove stopwords\n",
        "Finally we remove words that don't contain real meaning and are commonly used (e.g. 'this', 'the', 'a', etc.). \n",
        "\n",
        "\n",
        "Additionally, we remove unwanted characters (e.g., punctuation and numbers).\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f828c1d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download the punkt resource\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "#Define function with all necessary preprocessing steps for our IMDB reviews. As in week 9 we now use Stemming again.\n",
        "def preprocess(text):\n",
        "    # tokenize the text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # create stemmer object\n",
        "    stemmer = nltk.stem.PorterStemmer()\n",
        "\n",
        "    # stem each token\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    # get list of stopwords in English\n",
        "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "\n",
        "    # remove stopwords\n",
        "    filtered_tokens = [token for token in stemmed_tokens if token.lower() not in stopwords]\n",
        "    \n",
        "    # remove punctuation. Here we use another kind of removing punctuation compared to last week. The Regex-based removal also removes punctuation attached to words (e.g., \"hello,\" or \"test.\")\n",
        "    filtered_tokens_nopunct = [re.sub(r'[^\\w\\s]', '', token) for token in filtered_tokens if token]\n",
        "\n",
        "    return filtered_tokens_nopunct\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b0ddbc1",
      "metadata": {},
      "source": [
        "### Apply preprocessing\n",
        "\n",
        "After defining the different preprocessing steps, we now apply these preprocessing steps to our train and test set of our IMDB reviews. Running the code below we apply the preprocess function to the \"text\" column of our train and test set and save the new preprocessed reviews as a new column in our dataset named \"tokens\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0855faaf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply text preprocessing\n",
        "# Preprocess text data\n",
        "X_train['tokens'] = X_train['text'].apply(preprocess)\n",
        "X_test['tokens'] = X_test['text'].apply(preprocess)\n",
        "\n",
        "X_train['tokens'].iloc[0]  # Display first processed review"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7679c529",
      "metadata": {},
      "source": [
        "### Remove irrelevant words\n",
        "In this case, we manually remove specific words that are irrelevant to the analysis. The words we want to remove are \"movie\" and \"film\". As we already performed preprocessing and stemmed the original tokens, we also need to use the stemmed version of these two words.\n",
        "\n",
        "As the preprocess function returns a list of tokens instead an entire string, we can't use the replace function from Week 10. The replace function won't work as expected because it doesn't operate on individual elements within lists or substrings within strings. Instead, we need to process each list individually. The code below uses the apply function to iterate through each list in the column and filters out the unwanted tokens.\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4e100fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove additional irrelevant words (movie, film). \n",
        "X_train['tokens'] = X_train['tokens'].apply(lambda tokens: [token for token in tokens if token not in {'movi', 'film'}])\n",
        "X_test['tokens'] = X_test['tokens'].apply(lambda tokens: [token for token in tokens if token not in {'movi', 'film'}])\n",
        "\n",
        "X_train['tokens'].iloc[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1059705",
      "metadata": {},
      "source": [
        "## Topic Modeling\n",
        "\n",
        "In this section you will learn how to use topic modeling as an unsupervised method to analyse text data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b076811f",
      "metadata": {},
      "source": [
        "### Prepare the tokens\n",
        "\n",
        "In order to use the preprocessed tokens for our topic modeling, we first need to prepare the tokens and create a dictionary and a corpus.\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d4eef77c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim import corpora\n",
        "\n",
        "# Create dictionary and corpus for Gensim\n",
        "dictionary = corpora.Dictionary(X_train['tokens'])\n",
        "dictionary.filter_extremes(no_below=5)\n",
        "\n",
        "#the corpus shows which token appears how often in one review which can be used as input for the Topic Model\n",
        "corpus_train = [dictionary.doc2bow(text) for text in X_train['tokens']]\n",
        "corpus_test = [dictionary.doc2bow(text) for text in X_test['tokens']]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfd782f1",
      "metadata": {},
      "source": [
        "### Generate topic model (10 topics)\n",
        "\n",
        "After preparing our dataset, we can now calculate the topic model. We are aiming to find 10 topics in our corpus. This value can be found through an iterative process, where you start with a number topics and then evaluate the resulting topic model (are topics to close to each other or even overlapping?, are multiple topics contained within one big topic?) and adjust the number accordingly.\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7b0dd9d3-0c14-4fad-adf1-9fbfc3711c96",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b0dd9d3-0c14-4fad-adf1-9fbfc3711c96",
        "outputId": "9c18bcb8-666e-46ac-912a-4d32287be29c"
      },
      "outputs": [],
      "source": [
        "from gensim.models.ldamodel import LdaModel\n",
        "\n",
        "# Train LDA model\n",
        "k=10\n",
        "model_10 = LdaModel(corpus=corpus_train, num_topics=k, id2word = dictionary, iterations=100, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68837c55",
      "metadata": {
        "id": "68837c55"
      },
      "source": [
        "### Explore model\n",
        "We can also explore the topic model by looking at the top words per topic.\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "224aa8ed-7170-42d5-9f37-4081065d2bc1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "224aa8ed-7170-42d5-9f37-4081065d2bc1",
        "outputId": "6a1d6bc8-9e97-4dcd-e8b2-8e1ef7f793d3"
      },
      "outputs": [],
      "source": [
        "# Explore the topic model by printing the topics\n",
        "for topic_id, topic in model_10.print_topics(num_topics=10, num_words=10):\n",
        "    print(f\"Topic {topic_id + 1}: {topic}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "308f763e",
      "metadata": {},
      "source": [
        "#### Topic Prevalence\n",
        "\n",
        "We can also have a closer look at the *overall topic prevalence*, which helps prioritize the most dominant topics in the corpus. This is particularly useful for:\n",
        "\n",
        "1. Understanding the overall themes in your dataset.\n",
        "2. Identifying which topics have the most impact. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07c8dfb8",
      "metadata": {},
      "source": [
        "First we get an overview of how the topics are distributed over all reviews.\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0450ee7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get topic distributions for all reviews\n",
        "topic_distribution = [model_10.get_document_topics(bow) for bow in corpus_train]\n",
        "\n",
        "#print topic distributions for all reviews to get an overview\n",
        "for i, doc_distribution in enumerate(topic_distribution):\n",
        "    print(f\"Review {i+1}:\")\n",
        "    for topic_id, prob in doc_distribution:\n",
        "        print(f\"  Topic {topic_id+1}: {prob:.4f}\")\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "615e4bdb",
      "metadata": {},
      "source": [
        "We can also take a closer look at which reviews are most strongly associated with a specific topic.\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e62e9665",
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_id = 8  # For example, Topic 9 (adjust for zero-based indexing)\n",
        "n = 2  # Number of top documents to retrieve\n",
        "\n",
        "# Store document relevance scores\n",
        "doc_scores = []\n",
        "\n",
        "# Extract the probability for the target topic\n",
        "for i, doc_distribution in enumerate(topic_distribution):\n",
        "    topic_prob = dict(doc_distribution).get(topic_id, 0)\n",
        "    doc_scores.append((i, topic_prob))\n",
        "\n",
        "# Sort documents by their relevance to the target topic\n",
        "sorted_docs = sorted(doc_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Get the top `n` documents\n",
        "top_docs = sorted_docs[:n]\n",
        "\n",
        "# Print the results\n",
        "print(f\"Top {n} documents for Topic {topic_id+1}:\")\n",
        "for doc_index, prob in top_docs:\n",
        "    print(f\"Document {doc_index+1}: Score = {prob:.4f}\")\n",
        "    print(f\"Text: {X_train['text'][doc_index]}\")\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bd46e66",
      "metadata": {},
      "source": [
        "We now calculate the overall topic prevalence. Therefore we first need to define a function to calculate the topic prevalence (this is similar to the preprocess function we defined in the beginning). Afterwards we apply this function to our model to get the topic prevalence.\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "5c61335a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# define a function to calculate topic prevalence across the corpus\n",
        "def calculate_topic_prevalence(lda_model, corpus):\n",
        "\n",
        "    # Initialize an array to store the prevalence of each topic\n",
        "    topic_prevalence = np.zeros(lda_model.num_topics)\n",
        "    \n",
        "    # Get topic distribution for each document and sum the probabilities for each topic\n",
        "    for doc in corpus:\n",
        "        topic_distribution = lda_model.get_document_topics(doc)\n",
        "        for topic_id, prob in topic_distribution:\n",
        "            topic_prevalence[topic_id] += prob  # Sum the probabilities for each topic\n",
        "    \n",
        "    return topic_prevalence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "160ca72b",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Apply defined topic prevalence function and calculate topic prevalence for your topic model (model_10)\n",
        "topic_prevalence = calculate_topic_prevalence(model_10, corpus_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8111504a",
      "metadata": {},
      "source": [
        "We know can get an overview of the prevalence of each topic. For a better understanding and comparison we sort the topics according to their topic prevalence.\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b160a90",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sort the topics by prevalence in descending order\n",
        "sorted_topic_prevalence = sorted(enumerate(topic_prevalence), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print the sorted topic prevalence\n",
        "print(\"Topic Prevalence (Sorted):\")\n",
        "for topic_id, prevalence in sorted_topic_prevalence:\n",
        "    print(f\"Topic {topic_id+1}: {prevalence:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "790798cf",
      "metadata": {
        "id": "790798cf"
      },
      "source": [
        "#### Plot word cloud for a single topic\n",
        "\n",
        "Additionally we can plot word clouds for single topics.\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4b849fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to generate word cloud for a specific topic\n",
        "def generate_wordcloud_for_topic(lda_model, topic_id, num_words):\n",
        "    # Get the top words for the specified topic\n",
        "    topic_words = lda_model.show_topic(topic_id, topn=num_words)\n",
        "    \n",
        "    # Prepare the words and their probabilities for the word cloud\n",
        "    word_freq = {word: prob for word, prob in topic_words}\n",
        "    \n",
        "    # Generate the word cloud with the specified word frequencies\n",
        "    wc = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
        "    \n",
        "    # Plot the word cloud\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(wc, interpolation='bilinear')\n",
        "    plt.axis('off')  # Hide the axes\n",
        "    plt.title(f\"Word Cloud for Topic {topic_id+1}\")\n",
        "    plt.show()\n",
        "\n",
        "# Example: Generate word cloud for Topic 1\n",
        "generate_wordcloud_for_topic(model_10, topic_id=0, num_words=50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d49dbbe6",
      "metadata": {
        "id": "d49dbbe6"
      },
      "source": [
        "#### Print reviews most associated with a single topic\n",
        "\n",
        "Finally we can print out the reviews that are most associated with a single topic. Therefore, we first define a function to get the reviews per topic.\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "4800fb6c-bb17-433a-af31-77557cbf0001",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4800fb6c-bb17-433a-af31-77557cbf0001",
        "outputId": "ae4c1e1d-1404-4651-e668-8c39d3c8bb98"
      },
      "outputs": [],
      "source": [
        "#define a function to get the reviews per topic\n",
        "def get_reviews_by_topic(model, corpus, reviews, topic_id, threshold=0.5):\n",
        "    selected_reviews = []\n",
        "\n",
        "    # Iterate over the corpus and their corresponding document-topic distributions\n",
        "    for doc_idx, doc_topics in enumerate(model.get_document_topics(corpus)):\n",
        "        # Check the contribution of the specified topic\n",
        "        for topic, proportion in doc_topics:\n",
        "            if topic == topic_id and proportion >= threshold:\n",
        "                selected_reviews.append(reviews['text'].iloc[doc_idx])\n",
        "                break  # Stop checking other topics for this document\n",
        "\n",
        "    return selected_reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6b2d0c6",
      "metadata": {},
      "source": [
        "We can then apply the defined function to get the reviews for one specific topic. In this case, we will have a closer look at Topic 1 (the topic_id for Topic 1 is '0').\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "676502af",
      "metadata": {},
      "outputs": [],
      "source": [
        "#filtering on one specific topic ID and apply the function\n",
        "topic_id = 0\n",
        "reviews_for_topic = get_reviews_by_topic(model_10, corpus_train, X_train, topic_id, threshold=0.5)\n",
        "\n",
        "# Limit the number of reviews displayed to 10\n",
        "reviews_to_display = reviews_for_topic[:10]\n",
        "\n",
        "#print the reviews so that we can read them\n",
        "print(f\"Topic:{topic_id+1}\\n\")\n",
        "for review in reviews_to_display:\n",
        "    print(f\"{review}\\n\")\n",
        "    print(\"-\" * 80)  # Separator line for better readability"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bd64af7",
      "metadata": {
        "id": "8bd64af7"
      },
      "source": [
        "## Supervised Sentiment Analysis using Topic Modeling Features\n",
        "\n",
        "In the previous section we learned how we can use topic modeling as an unsupervised method to analyse textual data. Now we will use the created topic model as an additional feature in our supervised Sentiment Analysis with the aim to classify reviews as either positive or negative."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d77d50c",
      "metadata": {
        "id": "8d77d50c"
      },
      "source": [
        "### Store the per-document topic distributions in a dataframe for subsequent analysis.\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "5a2f881e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to extract per-document topic distributions\n",
        "def get_document_topic_distribution(model, corpus):\n",
        "    \"\"\"Get topic distributions for each document in a corpus.\"\"\"\n",
        "    return pd.DataFrame(\n",
        "        [\n",
        "            [prob for _, prob in model.get_document_topics(doc, minimum_probability=0)]\n",
        "            for doc in corpus\n",
        "        ],\n",
        "        columns=[f'Topic{i+1}' for i in range(model.num_topics)]\n",
        "    )\n",
        "\n",
        "train_topic_distributions = get_document_topic_distribution(model_10, corpus_train)\n",
        "test_topic_distributions = get_document_topic_distribution(model_10, corpus_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bca2f31e",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_topic_distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "312c3f1b",
      "metadata": {},
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00502e4c",
      "metadata": {
        "id": "00502e4c"
      },
      "source": [
        "#### Train random forest classifier\n",
        "\n",
        "We train a random forest classifier onthe training set (without hyperparameter tuning) to classify the sentiment based on the processed text features.\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "e2ee736b-5f06-4c50-a725-e2751d81cd70",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "e2ee736b-5f06-4c50-a725-e2751d81cd70",
        "outputId": "aac535ca-fef4-44bd-9f81-676c901b27a9"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Train a Random Forest classifier\n",
        "rf_topicmodel = RandomForestClassifier(random_state=42).fit(train_topic_distributions, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a18c396",
      "metadata": {
        "id": "1a18c396"
      },
      "source": [
        "#### Make predictions and calculate evaluation metrics on test set\n",
        "\n",
        "Similarly to last week, we can make predictions on the test set and calculate different evaluation metrics.\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e05e3e28-85ed-452a-85d4-ded97e6262e6",
      "metadata": {
        "id": "e05e3e28-85ed-452a-85d4-ded97e6262e6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "predictions_testset_rf_topicmodel = rf_topicmodel.predict_proba(test_topic_distributions)[:, 1]\n",
        "predictions_testset_rf_topicmodel_binary = np.where(predictions_testset_rf_topicmodel > 0.5, 1, 0)\n",
        "\n",
        "# Calculate Accuracy\n",
        "\n",
        "accuracy_rf = accuracy_score(y_test, predictions_testset_rf_topicmodel_binary)\n",
        "print(\"Accuracy (Random Forests):\", accuracy_rf)\n",
        "\n",
        "# Create the confusion matrix\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, predictions_testset_rf_topicmodel_binary)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4da0f190",
      "metadata": {
        "id": "4da0f190"
      },
      "source": [
        "#### ROC and Auc\n",
        "\n",
        "Plot ROC curve and calculate AUC on test set.\n",
        "With __binary__ classification we get relatively straight lines. With the classification __probability__ we can map the distribution better. That is why we use the classification probability (e.g., predictions_testset_rf_topicmodel) to calculate the AUC.\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZfxlV3bjXCub",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "ZfxlV3bjXCub",
        "outputId": "3f34f402-c470-4fff-f703-01633a1d2fc6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "\n",
        "# Calculate and Print the AUC score\n",
        "auc_score = roc_auc_score(y_test, predictions_testset_rf_topicmodel)\n",
        "print(\"AUC Score:\", auc_score)\n",
        "\n",
        "#plot ROC curve\n",
        "RocCurveDisplay.from_predictions(y_test, predictions_testset_rf_topicmodel, plot_chance_level=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0abd4583",
      "metadata": {
        "id": "0abd4583"
      },
      "source": [
        "### CART\n",
        "\n",
        "Instead of training a random forest we can also try out a basic decision tree and compare their performances. Therefor we simply grow a decision tree on the same training data as before and evaluate on the same test data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a85419c",
      "metadata": {},
      "source": [
        "\n",
        "### Train a simple classification tree and visualize it\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2368d5a-43ad-48a5-a78d-51238bb16340",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "a2368d5a-43ad-48a5-a78d-51238bb16340",
        "outputId": "3009325f-555d-405f-c1fa-960b4274880f"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "\n",
        "# Train Decision Tree classifier\n",
        "cart_topicmodel = DecisionTreeClassifier(min_samples_split=60, min_samples_leaf=20, max_depth=5, random_state=42).fit(train_topic_distributions, y_train)\n",
        "\n",
        "# Visualize Decision Tree\n",
        "plt.figure(figsize=(20,10))\n",
        "plot_tree(cart_topicmodel, feature_names=train_topic_distributions.columns, impurity=False, filled=True, rounded=True, proportion=True, class_names=True, fontsize=10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d3cdb44",
      "metadata": {
        "id": "4d3cdb44"
      },
      "source": [
        "#### Make predictions and calculate evaluation metrics on test set\n",
        "\n",
        "After calculating the decision tree's accuracy on the test set, we can see that there isn't a big difference in predictive performance, however the decision tree is a lot easier to interpret by a human compared to the random forest which is (without further analysis) a black box for us.\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ca824d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_testset_cart_topicmodel = cart_topicmodel.predict_proba(test_topic_distributions)[:, 1]\n",
        "predictions_testset_cart_topicmodel_binary = np.where(predictions_testset_cart_topicmodel > 0.5, 1, 0)\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy_cart = accuracy_score(y_test, predictions_testset_cart_topicmodel_binary)\n",
        "print(\"Accuracy (Random Forests):\", accuracy_cart)\n",
        "\n",
        "# Create the confusion matrix\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, predictions_testset_cart_topicmodel_binary)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d264655",
      "metadata": {},
      "source": [
        "#### ROC and Auc\n",
        "\n",
        "Plot ROC curve and calculate AUC on test set.\n",
        "With __binary__ classification we get relatively straight lines. With the classification __probability__ we can map the distribution better. That is why we use the classification probability (e.g., predictions_testset_cart_topicmodel) to calculate the AUC.\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f8c9f00",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate and Print the AUC score\n",
        "auc_score = roc_auc_score(y_test, predictions_testset_cart_topicmodel)\n",
        "print(\"AUC Score:\", auc_score)\n",
        "\n",
        "#plot ROC curve\n",
        "RocCurveDisplay.from_predictions(y_test, predictions_testset_cart_topicmodel, plot_chance_level=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cbbb3d1",
      "metadata": {
        "id": "4cbbb3d1"
      },
      "source": [
        "## Summary\n",
        "\n",
        "To sum it up let us have a look what we did in this week's tutorial:\n",
        "\n",
        "+ First we learned how to use Topic Modeling as an unsupervised method to analyse textual data\n",
        "+ Secondly we used the created topic model as an additional feature in our supervised Sentiment Analysis with the aim to classify reviews as either positive or negative\n",
        "\n",
        "You can use the cell below to perform and evaluate different sentiment analyses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a3dc3df",
      "metadata": {
        "id": "3a3dc3df"
      },
      "outputs": [],
      "source": [
        "# Enter your code here!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
