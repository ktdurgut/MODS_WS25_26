{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this week's tutorial we will switch from the regression setting to doing __classification__.  \n",
    "Therefore we will introduce __decision trees__ and use them to classify credit grants. Additionally we will look at methods to prevent __overfitting__ and metrics to evaluate our classification output.  \n",
    "Last but not least we will look at possible pitfalls when using certain metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import random\n",
    "# Seed\n",
    "np.random.seed(42)  # Set seed for NumPy\n",
    "random.seed(42) # Set seed for random module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/kbrennig/MODS_WS25_26/refs/heads/main/data/MicroCredit.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Explore data\n",
    "\n",
    "First let's have a look at the raw data.\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.describe())\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training and test sets\n",
    "\n",
    "We create a 80-20 training and test set. \n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dropping ID column\n",
    "data = data.drop(columns=\"ID\")\n",
    "\n",
    "data_training, data_test = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data\n",
    "\n",
    "### Transform variables\n",
    "\n",
    "To deal with categorical variables in the decision tree we need to dummy encode the variables.\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Dummy encoding / One-hot encoding\n",
    "\n",
    "categorical_columns = [\n",
    "    \"Tier\",\n",
    "    \"Build_Selfcon\",\n",
    "    \"Accommodation_Class\",\n",
    "    \"Loan_Type\",\n",
    "    \"Gender\",\n",
    "    \"Employment_Type\",\n",
    "    \"Doc_Proof_Inc\",\n",
    "    \"Marital_Status\",\n",
    "    \"Employer_Type\",\n",
    "    \"Education_Class\",\n",
    "    \"Mode_of_origin_class\"\n",
    "]\n",
    "\n",
    "# Creating the OneHotEncoder object\n",
    "encoder = OneHotEncoder(drop='first', handle_unknown='infrequent_if_exist', sparse_output=False)\n",
    "\n",
    "# Fit and transform the training data\n",
    "encoded_training = encoder.fit_transform(data_training.loc[:,categorical_columns])\n",
    "# Create a DataFrame with the encoded variables\n",
    "encoded_training_df = pd.DataFrame(encoded_training, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "# Join the encoded variables to the original DataFrame and remove the original columns\n",
    "data_training = data_training.reset_index(drop=True).join(encoded_training_df).drop(columns=categorical_columns)\n",
    "X_train = data_training.drop(columns=\"Decision\")\n",
    "y_train = data_training[\"Decision\"]\n",
    "\n",
    "\n",
    "# Transform the test data\n",
    "encoded_test = encoder.transform(data_test.loc[:,categorical_columns])\n",
    "encoded_test_df = pd.DataFrame(encoded_test, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "data_test = data_test.reset_index(drop=True).join(encoded_test_df).drop(columns=categorical_columns)\n",
    "X_test = data_test.drop(columns=\"Decision\")\n",
    "y_test = data_test[\"Decision\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Tree\n",
    "\n",
    "### Grow classification tree on training data\n",
    "\n",
    "We use the `DecisionTreeClassifier` object to create a classification tree and use the `fit` function to fit it to our training data. \n",
    "The fit function takes all columns of the `X_train` dataset as the independent variables to predict the outcome.\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model_01 = DecisionTreeClassifier(random_state=42).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlled tree\n",
    "\n",
    "As stated in the lecture, one potential drawback of decision trees is its tendency to easily __overfit__ if the tree size isn't controlled. Overfitting describes the issue of machine learning algorithms to tailor models to the training data, at the expense of generalization to previously unseen data points.  \n",
    "\n",
    "One solution to this problem can be to prevent the tree from growing to big by specifying certain parameters. \n",
    "\n",
    "The most commonly used parameters are:  \n",
    "\n",
    "* __min_samples_split:__ the minimum number of observations that must exist in a node in order for a split to be attempted.  \n",
    "* __min_samples_leaf:__ The minimum number of observations in any terminal node (leaf).  \n",
    "* __max_depth:__ The maximum depth of any node of the final tree, with the root node counted as depth 0.  \n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_02 = DecisionTreeClassifier(min_samples_split=60, min_samples_leaf=20, max_depth=5, random_state=42).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning\n",
    "\n",
    "Another option is called __pruning__ and it cuts off insignificant parts of the tree. To achieve this we simply specify the the parameter `ccp_alpha`. It is a complexity parameter, which defines a threshold for any split to be kept in the resulting tree. Roughly speaking it is the minimum improvement of model performance an additional split needs to provide. There are multiple other ways to prune a decision tree. \n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_03 = DecisionTreeClassifier(ccp_alpha=0.01, random_state=42).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print trees\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(120,40))\n",
    "# Plotting the tree\n",
    "# You can click on the plotted tree to zoom in and out.\n",
    "plot_tree(model_01, feature_names=X_train.columns, impurity=False, filled=True, rounded=True, proportion=True, class_names=True, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(model_02, feature_names=X_train.columns, impurity=False, filled=True, rounded=True, proportion=True, class_names=True, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plot_tree(model_03, feature_names=X_train.columns, impurity=False, filled=True, rounded=True, proportion=True, class_names=True, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Model_01\n",
    "\n",
    "#### Make predictions on test set\n",
    "\n",
    "Since we are doing classification this time, our evaluation is a little bit different compared to the last weeks.  \n",
    "\n",
    "Instead of predicting a numerical outcome variable we are now predicting probabilities which are then transformed into a binary label. In the `where(predictions_test_01[:,1] > 0.5, 1, 0)` statement we can see that we set the threshold at 0.5, so if the predicted probability for a granted credit is greater then 0.5 we are labeling it as granted and otherwise as rejected (0). \n",
    "\n",
    "You can think about how adjusting the threshold up or down would influence the outcome.\n",
    "\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test_01 = model_01.predict_proba(X_test)\n",
    "predictions_test_01_binary = np.where(predictions_test_01[:,1] > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the resulting accuracy and the confusion matrix\n",
    "\n",
    "Since we switched from regression to classification we are also using different evaluation metrics. The metric being calculated is the __accuracy__ which denotes how many of our predictions were correct. In addition to the accuracy we can also print a so called __confusion matrix__ which enables a more detailed interpretation of the errors we made.\n",
    "\n",
    "There are two possible errors in a binary classification setting:\n",
    "\n",
    "1. __False Positives (FP):__ The ground truth label is negative (0) but we predict it to be positive (1).\n",
    "2. __False Negatives (FN):__ The ground truth label is positive (1) but we predict it to be negative (0).\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy_model_01 = accuracy_score(y_test, predictions_test_01_binary)\n",
    "print(accuracy_model_01)\n",
    "\n",
    "# Display the confusion matrix\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, predictions_test_01_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the confusion matrix we can see that out of the 252 instances that we have in our test set, 197 are positive and 55 are negative. What does that mean for our model evaluation?  \n",
    "Since four out of five credits were granted, we would achieve an accuracy of 80% by simply predicting positive (1) for every instance. This problem is mainly due to the fact that we have an __unbalanced dataset__, meaning that one class has a lot more instances than the other.   \n",
    "\n",
    "There are different ways of dealing with this issue. We could for example choose an evaluation metric that takes this imbalance into consideration or we could even adjust our dataset by using methods like __under- or oversampling__.\n",
    "\n",
    "### Model_02\n",
    "\n",
    "#### Make predictions on test set and calculate the accuracy and the confusion matrix\n",
    "\n",
    "We can repeat the calculations for the second model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test_02 = model_02.predict_proba(X_test)\n",
    "predictions_test_02_binary = np.where(predictions_test_02[:,1] > 0.5, 1, 0)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy_model_02 = accuracy_score(y_test, predictions_test_02_binary)\n",
    "print(accuracy_model_02)\n",
    "\n",
    "# Display the confusion matrix\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, predictions_test_02_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model_03\n",
    "\n",
    "#### Make predictions on test set and calculate evaluation metrics\n",
    "\n",
    "*Adapt the code and output the confusion matrix created by the third model (model_03) on the test set*\n",
    "\n",
    "*Write your code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC and Auc\n",
    "\n",
    "![ROC_Curve](https://raw.githubusercontent.com/kbrennig/MODS_WS24_25/refs/heads/main/Week_6_exercise/images/ROC_Curve.png)\n",
    "<br>\n",
    "Source: Provost & Fawcett (2013)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another evaluation metric for classification tasks is the __Area under the curve (AUC)__. The AUC is calculated using the __Receiver operating characteristic (ROC)__ curve which is obtained by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.\n",
    "\n",
    "### Plot ROC curve and calculate AUC on test set  \n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "# Calculate and Print the AUC score\n",
    "auc_score = roc_auc_score(y_test, predictions_test_01[:,1])\n",
    "print(\"AUC Score:\", auc_score)\n",
    "\n",
    "#plot ROC curve\n",
    "RocCurveDisplay.from_predictions(y_test, predictions_test_01[:,1], plot_chance_level=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Adapt the code and calculate the AUC of the second model (model_02) on the test data*\n",
    "\n",
    "*Write your code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "So to sum it up let us have a look what we did in this week's tutorial:\n",
    "\n",
    "1. we learned how to use __decision trees__ to predict a __categorical output__ variable.\n",
    "2. after that, we had a look at two methods that can be used to reduce __overfitting__.\n",
    "3. finally we looked at __metrics__ to evaluate our classification output and how to avoid pitfalls when the data is __imbalanced__.\n",
    "\n",
    "\n",
    "\n",
    "*You can use the cell below to build and evaluate different models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-notebooks-3-11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
